{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch_optimizer import Lookahead\n",
    "from src.data.config import dataset, data_loader, model as model_config, optimizer as optimizer_config, scheduler as scheduler_config, training\n",
    "\n",
    "# Import custom modules\n",
    "from src.models.model import KeywordSpottingModel_with_cls\n",
    "from src.data.data_loader import load_speech_commands_dataset, TFDatasetAdapter, load_bg_noise_dataset\n",
    "from src.utils.utils import set_memory_GB, print_model_size, log_to_file, plot_learning_curves,EarlyStopping\n",
    "from src.utils.augmentations import add_time_shift_and_align, add_silence\n",
    "from src.utils.train_utils import trainig_loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load datasets\n",
    "train_ds, val_ds, test_ds, silence_ds , info = load_speech_commands_dataset(reduced=False)\n",
    "bg_noise_ds = load_bg_noise_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 11:32:41.896014: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-01-07 11:32:43.533445: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize datasets with configurations\n",
    "pytorch_train_dataset = TFDatasetAdapter(train_ds, bg_noise_ds, **dataset, augmentation=[lambda x: add_time_shift_and_align(x)])\n",
    "pytorch_val_dataset = TFDatasetAdapter(val_ds, None, **dataset, augmentation=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(pytorch_train_dataset, **data_loader, shuffle=True)\n",
    "val_loader = DataLoader(pytorch_val_dataset, **data_loader, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize model\n",
    "model = KeywordSpottingModel_with_cls(**model_config).to(\"cuda\")\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss().to(\"cuda\")\n",
    "\n",
    "# Optimizer\n",
    "base_optimizer = optim.Adam(model.parameters(), lr=optimizer_config['lr'], weight_decay=optimizer_config['weight_decay'])\n",
    "optimizer = Lookahead(base_optimizer, **optimizer_config['lookahead'])\n",
    "\n",
    "# Scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, **scheduler_config['reduce_lr_on_plateau'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 333892\n"
     ]
    }
   ],
   "source": [
    "model_size = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model size: {model_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Training loop\n",
    "# num_epochs = training['num_epochs']\n",
    "# try:\n",
    "#     train_accuracies, val_accuracies, train_losses, val_losses = trainig_loop(model, num_epochs, train_loader, val_loader, criterion, optimizer, scheduler)\n",
    "#     plot_learning_curves(train_accuracies, val_accuracies, train_losses, val_losses, save_to_file=True)\n",
    "# except Exception as err:\n",
    "#     log_to_file(str(err))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_798969/1833195287.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 89.20245398773007%\n"
     ]
    }
   ],
   "source": [
    "# load test data\n",
    "pytorch_test_dataset = TFDatasetAdapter(test_ds, None, **dataset, augmentation=None)\n",
    "test_loader = DataLoader(pytorch_test_dataset, **data_loader, shuffle=False)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for audio, labels in test_loader:\n",
    "        audio, labels = audio.to(\"cuda\"), labels.to(\"cuda\")\n",
    "        outputs = model(audio)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        accuracy += (predicted == labels).sum().item()\n",
    "test_accuracy = 100 * accuracy / total\n",
    "print(f'Test Accuracy: {test_accuracy}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 669/669 [13:00<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "from torch.quantization import quantize_qat\n",
    "from tqdm import tqdm\n",
    "model.train()\n",
    "model.qconfig = torch.quantization.get_default_qconfig(\"fbgemm\")  # Specify the backend\n",
    "torch.quantization.prepare_qat(model, inplace=True)\n",
    "import torch.quantization as quant\n",
    "\n",
    "# Specify the quantization configuration\n",
    "model.qconfig = quant.QConfig(\n",
    "    activation=quant.default_observer,  # Observer for activations\n",
    "    weight=quant.default_weight_observer  # Use per_tensor_affine for weights\n",
    ")\n",
    "\n",
    "# Prepare the model for quantization\n",
    "quant.prepare(model, inplace=True)\n",
    "\n",
    "# Calibrate or fine-tune the model (if needed)\n",
    "\n",
    "# Fine-tune the model\n",
    "for epoch in range(1):\n",
    "    for audio, labels in tqdm(train_loader):\n",
    "        audio, labels = audio.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(audio)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unsupported qscheme: per_channel_affine",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m quant\u001b[38;5;241m.\u001b[39mprepare(model, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Calibrate or fine-tune the model (if needed)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Convert the model to quantized form\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mquant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/quantize.py:563\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(module, mapping, inplace, remove_qconfig, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace:\n\u001b[1;32m    562\u001b[0m     module \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(module)\n\u001b[0;32m--> 563\u001b[0m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remove_qconfig:\n\u001b[1;32m    568\u001b[0m     _remove_qconfig(module)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/quantize.py:607\u001b[0m, in \u001b[0;36m_convert\u001b[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    603\u001b[0m        type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping:\n\u001b[1;32m    604\u001b[0m         _convert(mod, mapping, \u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# inplace\u001b[39;00m\n\u001b[1;32m    605\u001b[0m                  is_reference, convert_custom_config_dict,\n\u001b[1;32m    606\u001b[0m                  use_precomputed_fake_quant\u001b[38;5;241m=\u001b[39muse_precomputed_fake_quant)\n\u001b[0;32m--> 607\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m \u001b[43mswap_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_module_class_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    610\u001b[0m     module\u001b[38;5;241m.\u001b[39m_modules[key] \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/quantize.py:642\u001b[0m, in \u001b[0;36mswap_module\u001b[0;34m(mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant)\u001b[0m\n\u001b[1;32m    640\u001b[0m sig \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(qmod\u001b[38;5;241m.\u001b[39mfrom_float)\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_precomputed_fake_quant\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sig\u001b[38;5;241m.\u001b[39mparameters:\n\u001b[0;32m--> 642\u001b[0m     new_mod \u001b[38;5;241m=\u001b[39m \u001b[43mqmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_float\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    644\u001b[0m     new_mod \u001b[38;5;241m=\u001b[39m qmod\u001b[38;5;241m.\u001b[39mfrom_float(mod)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/ao/nn/quantized/modules/linear.py:286\u001b[0m, in \u001b[0;36mLinear.from_float\u001b[0;34m(cls, mod, use_precomputed_fake_quant)\u001b[0m\n\u001b[1;32m    282\u001b[0m qweight \u001b[38;5;241m=\u001b[39m _quantize_weight(mod\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mfloat(), weight_post_process)\n\u001b[1;32m    283\u001b[0m qlinear \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(mod\u001b[38;5;241m.\u001b[39min_features,\n\u001b[1;32m    284\u001b[0m               mod\u001b[38;5;241m.\u001b[39mout_features,\n\u001b[1;32m    285\u001b[0m               dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m--> 286\u001b[0m \u001b[43mqlinear\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_weight_bias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m qlinear\u001b[38;5;241m.\u001b[39mscale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(act_scale)\n\u001b[1;32m    288\u001b[0m qlinear\u001b[38;5;241m.\u001b[39mzero_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(act_zp)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/ao/nn/quantized/modules/linear.py:241\u001b[0m, in \u001b[0;36mLinear.set_weight_bias\u001b[0;34m(self, w, b)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_weight_bias\u001b[39m(\u001b[38;5;28mself\u001b[39m, w: torch\u001b[38;5;241m.\u001b[39mTensor, b: Optional[torch\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_packed_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_weight_bias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/ao/nn/quantized/modules/linear.py:33\u001b[0m, in \u001b[0;36mLinearPackedParams.set_weight_bias\u001b[0;34m(self, weight, bias)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mexport\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_weight_bias\u001b[39m(\u001b[38;5;28mself\u001b[39m, weight: torch\u001b[38;5;241m.\u001b[39mTensor, bias: Optional[torch\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mqint8:\n\u001b[0;32m---> 33\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_prepack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mquantized\u001b[38;5;241m.\u001b[39mlinear_prepack_fp16(weight, bias)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_ops.py:1061\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m self_\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[1;32m   1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(self_, args, kwargs)\n\u001b[0;32m-> 1061\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unsupported qscheme: per_channel_affine"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert the model to quantized form\n",
    "quant.convert(model, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # save model cpu map\n",
    "# model.cpu()\n",
    "torch.save(model.state_dict(), 'quntizeAwareModel.pth', map_location=torch.device(device))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_784758/3196348652.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"quntizeAwareModel.pth\"), strict=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"quntizeAwareModel.pth\"), strict=True, map_location=torch.device(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# load test data\u001b[39;00m\n\u001b[1;32m      3\u001b[0m pytorch_test_dataset \u001b[38;5;241m=\u001b[39m TFDatasetAdapter(test_ds, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset, augmentation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# model.to(\"cpu\")\n",
    "# load test data\n",
    "pytorch_test_dataset = TFDatasetAdapter(test_ds, None, **dataset, augmentation=None)\n",
    "test_loader = DataLoader(pytorch_test_dataset, **data_loader, shuffle=False)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for audio, labels in test_loader:\n",
    "        audio, labels = audio.to(\"cuda\"), labels.to(\"cuda\")\n",
    "        outputs = model(audio)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        accuracy += (predicted == labels).sum().item()\n",
    "test_accuracy = 100 * accuracy / total\n",
    "print(f'Test Accuracy: {test_accuracy}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 1 sample from test data\n",
    "audio, label = next(iter(test_loader))\n",
    "audio, label = audio.to(device), label.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/mambaPy/mamba.py/mambapy/pscan.py:18: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  return 2 ** math.ceil(math.log2(len))\n",
      "/workspace/mambaPy/mamba.py/mambapy/pscan.py:168: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if L == npo2(L):\n",
      "/workspace/mambaPy/mamba.py/mambapy/pscan.py:49: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  num_steps = int(math.log2(L))\n",
      "/workspace/mambaPy/mamba.py/mambapy/pscan.py:66: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if Xa.size(2) == 4:\n"
     ]
    }
   ],
   "source": [
    "dummy_input = audio[0].unsqueeze(0)\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"model.onnx\",\n",
    "    export_params=True,\n",
    "    opset_version=16,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size = 1 testloader\n",
    "test_loader = DataLoader(pytorch_test_dataset, batch_size=10, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85511"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_train_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare calibration data\n",
    "calibration_data = []\n",
    "calibrate_data_loader = DataLoader(pytorch_train_dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "for audio, _ in calibrate_data_loader:\n",
    "    audio_np = audio.numpy()  # Convert PyTorch tensors to NumPy arrays\n",
    "    # Ensure the feature dimensions are [69, 135]\n",
    "    reshaped_audio = audio_np.reshape(-1, 69, 135)\n",
    "    calibration_data.extend(reshaped_audio)\n",
    "\n",
    "    # Stop after collecting enough samples for calibration\n",
    "    if len(calibration_data) >= 5000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization import CalibrationDataReader\n",
    "\n",
    "class MyCalibrationDataReader(CalibrationDataReader):\n",
    "    def __init__(self, calibration_data, input_name):\n",
    "        \"\"\"\n",
    "        Initialize the data reader.\n",
    "        :param calibration_data: List of calibration samples (e.g., images or tensors).\n",
    "        :param input_name: Name of the input node in the ONNX model.\n",
    "        \"\"\"\n",
    "        self.data = calibration_data\n",
    "        self.index = 0\n",
    "        self.input_name = input_name\n",
    "\n",
    "    def get_next(self):\n",
    "        \"\"\"\n",
    "        Retrieve the next batch of data for calibration.\n",
    "        :return: Dictionary of {input_name: input_data_batch}.\n",
    "        \"\"\"\n",
    "        if self.index < len(self.data):\n",
    "            batch = self.data[self.index:self.index + 1]  # ONNX expects batched inputs\n",
    "            self.index += 1\n",
    "            return {self.input_name: batch}\n",
    "        return None  # Signal the end of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensor data and making histogram ...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_static, QuantType, CalibrationMethod\n",
    "\n",
    "quantize_static(\n",
    "    \"model.onnx\",\n",
    "    \"quantized_model.onnx\",\n",
    "    weight_type=QuantType.QFLOAT8E4M3FN,\n",
    "    nodes_to_exclude=[\"/mamba_layers.0/Slice_3\", \"/mamba_layers.1/Slice_3\"],\n",
    "    calibration_data_reader=MyCalibrationDataReader(calibration_data, \"input\"),\n",
    "    calibrate_method = CalibrationMethod.Distribution,\n",
    "    use_external_data_format=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized ONNX model is valid!\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# Load and check the quantized ONNX model\n",
    "quantized_model_path = \"quantized_model.onnx\"\n",
    "onnx_model = onnx.load(quantized_model_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"Quantized ONNX model is valid!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model output: [array([[ 0.12826443,  0.25652885,  0.5130577 ,  0.38479328,  0.76958656,\n",
      "         0.897851  , -0.25652885,  0.64132214,  1.4109087 , -0.12826443,\n",
      "         0.897851  ,  0.25652885]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# Load the quantized model\n",
    "session = ort.InferenceSession(\"quantized_model.onnx\")\n",
    "\n",
    "# Get model input and output names\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "\n",
    "# Test inference with sample data\n",
    "dummy_input = audio[0].unsqueeze(0).numpy()\n",
    "outputs = session.run([output_name], {input_name: dummy_input})\n",
    "print(\"Quantized model output:\", outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 39/39 [01:44<00:00,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model accuracy: 5.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "correct = 0\n",
    "total = 0\n",
    "test_loader = DataLoader(pytorch_test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "for audio, labels in tqdm(test_loader):\n",
    "    audio = audio.numpy()\n",
    "    outputs = session.run([output_name], {input_name: audio})\n",
    "    predicted = np.argmax(outputs[0], axis=1)\n",
    "    total += labels.shape[0]\n",
    "    correct += (predicted == labels.numpy()).sum()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Quantized model accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv1d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "\n",
      "MACs: 650593536.0 Which are 0.650593536 Giga-MACs, Params: 151242.0\n",
      "\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv1d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "\n",
      "MACs: 20331048.0 Which are 0.020331048 Giga-MACs, Params: 151242.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils import compute_inference_GPU_mem\n",
    "\n",
    "# Define missing variables\n",
    "configs = {'batch_size': 32}  # Example value, adjust as needed\n",
    "input_dim = model_config['input_dim']  # Example value, adjust as needed\n",
    "d_model = model_config['d_model']  # Example value, adjust as needed\n",
    "d_state = model_config['d_state']  # Example value, adjust as needed\n",
    "d_conv = model_config['d_conv']  # Example value, adjust as needed\n",
    "expand = model_config['expand']  # Example value, adjust as needed\n",
    "\n",
    "# Save model size(macs, params) and accuracy\n",
    "batch_size = configs['batch_size']\n",
    "macs, params = print_model_size(model, input_size=torch.randn(batch_size, input_dim, d_model-1).to(\"cuda\"))\n",
    "macs = macs / 1e9\n",
    "accuracy = test_accuracy\n",
    "data = {'Model': ['KeywordSpottingModel_RSM_Norm_0-1-2_order_cls_bgnoise'], 'GMACs': [macs], 'Params': [params], 'Accuracy': [accuracy]}\n",
    "model_config = {'input_dim': input_dim, 'd_model': d_model, 'd_state': d_state, 'd_conv': d_conv, 'expand': expand}\n",
    "data.update(model_config)\n",
    "inf_GPU_mem = compute_inference_GPU_mem(model, input=torch.randn(1, input_dim, d_model-1).to(\"cuda\"))\n",
    "# Inference macs and params\n",
    "inf_macs, inf_params = print_model_size(model, input_size=torch.randn(1, input_dim, d_model-1).to(\"cuda\"))\n",
    "inference_data = {'Inference CUDA Mem in MB': [inf_GPU_mem], 'Inference GMACs': [inf_macs / 1e9], 'Inference Params': [inf_params]}\n",
    "data.update(inference_data)\n",
    "df = pd.DataFrame(data, index=[0])\n",
    "df.to_csv('results.csv', mode='a', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
